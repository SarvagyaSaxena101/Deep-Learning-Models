{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.11.11","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"gpu","dataSources":[],"dockerImageVersionId":31011,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":true}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import torch\nimport torch.nn as nn\nfrom torch.utils.data import DataLoader\nfrom datasets import load_dataset\nfrom transformers import MarianTokenizer\nimport math\n\n# Hyperparameters\nSRC_LANG = 'en'\nTGT_LANG = 'de'\nBATCH_SIZE = 32\nMAX_LEN = 128\nSRC_VOCAB_SIZE = tokenizer.vocab_size  # assuming same tokenizer\nTGT_VOCAB_SIZE = tokenizer.vocab_size\n\nEMB_SIZE = 512\nNHEAD = 8\nFFN_HID_DIM = 512\nBATCH_SIZE = 32\nNUM_ENCODER_LAYERS = 3\nNUM_DECODER_LAYERS = 3\nDROPOUT = 0.1\n\nNUM_EPOCHS = 10\nDEVICE = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n\n# 1. Load Dataset\ndataset = load_dataset(\"wmt16\", \"de-en\")\ntrain_data = dataset['train'].select(range(10000))  # Subset for speed\nval_data = dataset['validation']\n\n# 2. Tokenizer\ntokenizer = MarianTokenizer.from_pretrained(\"Helsinki-NLP/opus-mt-en-de\")\n\ndef tokenize(example):\n    inputs = tokenizer(example['translation'][SRC_LANG],\n                       padding='max_length', truncation=True,\n                       max_length=MAX_LEN)\n    targets = tokenizer(example['translation'][TGT_LANG],\n                        padding='max_length', truncation=True,\n                        max_length=MAX_LEN)\n    return {\n        'input_ids': inputs['input_ids'],\n        'attention_mask': inputs['attention_mask'],\n        'labels': targets['input_ids']\n    }\n\n# Tokenize dataset\ntrain_data = train_data.map(tokenize, remove_columns=[\"translation\"])\nval_data = val_data.map(tokenize, remove_columns=[\"translation\"])\n\n# 3. Data Collator (Fix: convert lists to tensors)\ndef collate_fn(batch):\n    input_ids = torch.tensor([item['input_ids'] for item in batch], dtype=torch.long)\n    attention_mask = torch.tensor([item['attention_mask'] for item in batch], dtype=torch.long)\n    labels = torch.tensor([item['labels'] for item in batch], dtype=torch.long)\n    return {\n        'input_ids': input_ids,\n        'attention_mask': attention_mask,\n        'labels': labels\n    }\n\n# 4. DataLoader\ntrain_loader = DataLoader(train_data, batch_size=BATCH_SIZE, shuffle=True, collate_fn=collate_fn)\n\n# 5. Positional Encoding\nclass PositionalEncoding(nn.Module):\n    def __init__(self, emb_size, dropout=0.1, maxlen=5000):\n        super().__init__()\n        self.dropout = nn.Dropout(p=dropout)\n        pe = torch.zeros(maxlen, emb_size)\n        position = torch.arange(0, maxlen, dtype=torch.float).unsqueeze(1)\n        div_term = torch.exp(torch.arange(0, emb_size, 2).float() * (-math.log(10000.0) / emb_size))\n        pe[:, 0::2] = torch.sin(position * div_term)\n        pe[:, 1::2] = torch.cos(position * div_term)\n        self.pe = pe.unsqueeze(0)\n\n    def forward(self, x):\n        x = x + self.pe[:, :x.size(1)].to(x.device)\n        return self.dropout(x)\n\n# 6. Transformer Model\nclass Seq2SeqTransformer(nn.Module):\n    def __init__(\n        self, num_encoder_layers, num_decoder_layers, emb_size, \n        nhead, src_vocab_size, tgt_vocab_size, dim_feedforward=512, dropout=0.1\n    ):\n        super(Seq2SeqTransformer, self).__init__()\n        self.transformer = nn.Transformer(\n            d_model=emb_size, nhead=nhead, \n            num_encoder_layers=num_encoder_layers,\n            num_decoder_layers=num_decoder_layers, \n            dim_feedforward=dim_feedforward, \n            dropout=dropout, batch_first=True\n        )\n        self.src_tok_emb = nn.Embedding(src_vocab_size, emb_size)\n        self.tgt_tok_emb = nn.Embedding(tgt_vocab_size, emb_size)\n        self.pos_encoder = nn.Sequential(\n            PositionalEncoding(emb_size, dropout)\n        )\n        self.pos_decoder = nn.Sequential(\n            PositionalEncoding(emb_size, dropout)\n        )\n        self.generator = nn.Linear(emb_size, tgt_vocab_size)\n\n    def forward(self, src, tgt, src_mask, tgt_mask,\n                src_padding_mask, tgt_padding_mask, memory_key_padding_mask):\n        src_emb = self.pos_encoder(self.src_tok_emb(src))\n        tgt_emb = self.pos_decoder(self.tgt_tok_emb(tgt))\n        outs = self.transformer(\n            src_emb, tgt_emb, src_mask, tgt_mask, \n            None, src_padding_mask, tgt_padding_mask, memory_key_padding_mask\n        )\n        return self.generator(outs)\n\n# 7. Initialize model\nVOCAB_SIZE = tokenizer.vocab_size\nmodel = Seq2SeqTransformer(\n    num_encoder_layers=NUM_ENCODER_LAYERS,\n    num_decoder_layers=NUM_DECODER_LAYERS,\n    emb_size=EMB_SIZE,\n    nhead=NHEAD,\n    src_vocab_size=SRC_VOCAB_SIZE,\n    tgt_vocab_size=TGT_VOCAB_SIZE,\n    dim_feedforward=FFN_HID_DIM,\n    dropout=DROPOUT\n).to(DEVICE)\n\noptimizer = torch.optim.Adam(model.parameters(), lr=0.0001)\nloss_fn = nn.CrossEntropyLoss(ignore_index=tokenizer.pad_token_id)\n\n# 8. Mask generation\ndef generate_square_subsequent_mask(sz):\n    return torch.triu(torch.ones(sz, sz) * float('-inf'), diagonal=1)\n\n# 9. Padding mask generation function\ndef create_padding_mask(input_ids, pad_token_id):\n    return (input_ids == pad_token_id)\n\n# 10. Training loop\nfor epoch in range(NUM_EPOCHS):\n    model.train()\n    total_loss = 0\n\n    for batch in train_loader:\n        src = batch['input_ids'].to(DEVICE)\n        tgt = batch['labels'].to(DEVICE)\n\n        tgt_input = tgt[:, :-1]\n        tgt_out = tgt[:, 1:]\n\n        tgt_mask = generate_square_subsequent_mask(tgt_input.size(1)).to(DEVICE)\n\n        # Generate padding masks for both source and target sequences\n        src_padding_mask = create_padding_mask(src, tokenizer.pad_token_id).to(DEVICE)\n        tgt_padding_mask = create_padding_mask(tgt_input, tokenizer.pad_token_id).to(DEVICE)\n        memory_key_padding_mask = create_padding_mask(src, tokenizer.pad_token_id).to(DEVICE)\n\n        logits = model(src, tgt_input, src_mask=None, tgt_mask=tgt_mask,\n                       src_padding_mask=src_padding_mask, tgt_padding_mask=tgt_padding_mask,\n                       memory_key_padding_mask=memory_key_padding_mask)\n\n        loss = loss_fn(logits.reshape(-1, logits.shape[-1]), tgt_out.reshape(-1))\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n        total_loss += loss.item()\n\n    avg_loss = total_loss / len(train_loader)\n    print(f\"Epoch {epoch+1}/{NUM_EPOCHS}, Loss: {avg_loss:.4f}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:59:47.816368Z","iopub.execute_input":"2025-04-19T06:59:47.817275Z","iopub.status.idle":"2025-04-19T07:12:31.868867Z","shell.execute_reply.started":"2025-04-19T06:59:47.817243Z","shell.execute_reply":"2025-04-19T07:12:31.868202Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.11/dist-packages/torch/nn/functional.py:5849: UserWarning: Support for mismatched key_padding_mask and attn_mask is deprecated. Use same type for both instead.\n  warnings.warn(\n","output_type":"stream"},{"name":"stdout","text":"Epoch 1/10, Loss: 6.3220\nEpoch 2/10, Loss: 4.5985\nEpoch 3/10, Loss: 3.9972\nEpoch 4/10, Loss: 3.6712\nEpoch 5/10, Loss: 3.4506\nEpoch 6/10, Loss: 3.2839\nEpoch 7/10, Loss: 3.1484\nEpoch 8/10, Loss: 3.0289\nEpoch 9/10, Loss: 2.9231\nEpoch 10/10, Loss: 2.8235\n","output_type":"stream"}],"execution_count":8},{"cell_type":"code","source":"pip install sacrebleu\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T06:55:58.957361Z","iopub.execute_input":"2025-04-19T06:55:58.957878Z","iopub.status.idle":"2025-04-19T06:56:03.075390Z","shell.execute_reply.started":"2025-04-19T06:55:58.957827Z","shell.execute_reply":"2025-04-19T06:56:03.074614Z"}},"outputs":[{"name":"stdout","text":"Collecting sacrebleu\n  Downloading sacrebleu-2.5.1-py3-none-any.whl.metadata (51 kB)\n\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m51.8/51.8 kB\u001b[0m \u001b[31m2.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hCollecting portalocker (from sacrebleu)\n  Downloading portalocker-3.1.1-py3-none-any.whl.metadata (8.6 kB)\nRequirement already satisfied: regex in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (2024.11.6)\nRequirement already satisfied: tabulate>=0.8.9 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.9.0)\nRequirement already satisfied: numpy>=1.17 in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (1.26.4)\nRequirement already satisfied: colorama in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (0.4.6)\nRequirement already satisfied: lxml in /usr/local/lib/python3.11/dist-packages (from sacrebleu) (5.3.1)\nRequirement already satisfied: mkl_fft in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.3.8)\nRequirement already satisfied: mkl_random in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (1.2.4)\nRequirement already satisfied: mkl_umath in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (0.1.1)\nRequirement already satisfied: mkl in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2025.1.0)\nRequirement already satisfied: tbb4py in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: mkl-service in /usr/local/lib/python3.11/dist-packages (from numpy>=1.17->sacrebleu) (2.4.1)\nRequirement already satisfied: intel-openmp<2026,>=2024 in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: tbb==2022.* in /usr/local/lib/python3.11/dist-packages (from mkl->numpy>=1.17->sacrebleu) (2022.1.0)\nRequirement already satisfied: tcmlib==1.* in /usr/local/lib/python3.11/dist-packages (from tbb==2022.*->mkl->numpy>=1.17->sacrebleu) (1.2.0)\nRequirement already satisfied: intel-cmplr-lib-rt in /usr/local/lib/python3.11/dist-packages (from mkl_umath->numpy>=1.17->sacrebleu) (2024.2.0)\nRequirement already satisfied: intel-cmplr-lib-ur==2024.2.0 in /usr/local/lib/python3.11/dist-packages (from intel-openmp<2026,>=2024->mkl->numpy>=1.17->sacrebleu) (2024.2.0)\nDownloading sacrebleu-2.5.1-py3-none-any.whl (104 kB)\n\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m104.1/104.1 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n\u001b[?25hDownloading portalocker-3.1.1-py3-none-any.whl (19 kB)\nInstalling collected packages: portalocker, sacrebleu\nSuccessfully installed portalocker-3.1.1 sacrebleu-2.5.1\nNote: you may need to restart the kernel to use updated packages.\n","output_type":"stream"}],"execution_count":2},{"cell_type":"code","source":"import torch\nfrom transformers import AutoTokenizer, AutoModelForSeq2SeqLM\nimport sacrebleu\n\n# Define translation function\ndef translate(model, tokenizer, src_sentence, device, max_len=128):\n    model.eval()\n    \n    # Tokenize and prepare the source sentence\n    src_input = tokenizer(src_sentence, return_tensors='pt', padding=True, truncation=True, max_length=max_len)\n    src_input_ids = src_input['input_ids'].to(device)\n    src_attention_mask = src_input['attention_mask'].to(device)\n    \n    # Generate translation using model's built-in method\n    generated_ids = model.generate(input_ids=src_input_ids, attention_mask=src_attention_mask, max_length=max_len)\n    \n    # Decode the output tokens into a sentence\n    output_sentence = tokenizer.decode(generated_ids[0], skip_special_tokens=True)\n    return output_sentence\n\n# Define BLEU score computation function\ndef compute_bleu(model, tokenizer, dataset, num_samples=100):\n    references = []\n    hypotheses = []\n    \n    # Loop through dataset to compute BLEU score\n    for i in range(num_samples):\n        src = dataset[i]['translation']['en']\n        ref = dataset[i]['translation']['de']\n        \n        pred = translate(model, tokenizer, src, device)\n\n        references.append([ref])  # sacrebleu expects a list of references\n        hypotheses.append(pred)\n\n    # Compute BLEU score\n    bleu = sacrebleu.corpus_bleu(hypotheses, references)\n    print(f\"\\nFinal BLEU score: {bleu.score:.2f}\")\n    return bleu.score\n\n# Example usage\nif __name__ == \"__main__\":\n    # Define device\n    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n    \n    # Load the tokenizer and model\n    model_name = 't5-small'  # Example: using T5 model; replace with your model\n    tokenizer = AutoTokenizer.from_pretrained(model_name)\n    model = AutoModelForSeq2SeqLM.from_pretrained(model_name).to(device)  # Assuming CUDA (GPU) is available\n    \n    # Example dataset (replace with your actual dataset)\n    dataset = [\n        {'translation': {'en': 'Hello, how are you?', 'de': 'Hallo, wie geht es dir?'}},\n        {'translation': {'en': 'What is your name?', 'de': 'Wie heißt du?'}}\n        # Add more samples as needed\n    ]\n    \n    # Compute BLEU score on test set\n    bleu_score = compute_bleu(model, tokenizer, dataset, num_samples=2)\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T07:17:19.096136Z","iopub.execute_input":"2025-04-19T07:17:19.096607Z","iopub.status.idle":"2025-04-19T07:17:24.861473Z","shell.execute_reply.started":"2025-04-19T07:17:19.096584Z","shell.execute_reply":"2025-04-19T07:17:24.860697Z"}},"outputs":[{"name":"stdout","text":"\nFinal BLEU score: 8.17\n","output_type":"stream"}],"execution_count":17},{"cell_type":"code","source":"import torch\n\ndef translate_english_to_french(model, tokenizer, sentence: str, device='cuda' if torch.cuda.is_available() else 'cpu', max_length=50):\n    model.eval()  # Set model to evaluation mode\n    \n    # Prepare input text with task prefix if needed (like T5 usually requires)\n    input_text = \"translate English to French: \" + sentence\n\n    # Tokenize input\n    inputs = tokenizer.encode(input_text, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n\n    # Generate output ids\n    with torch.no_grad():\n        outputs = model.generate(\n            inputs,\n            max_length=max_length,\n            num_beams=4,\n            early_stopping=True\n        )\n\n    # Decode and return\n    translated_text = tokenizer.decode(outputs[0], skip_special_tokens=True)\n    return translated_text\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T07:44:10.058107Z","iopub.execute_input":"2025-04-19T07:44:10.058927Z","iopub.status.idle":"2025-04-19T07:44:10.064572Z","shell.execute_reply.started":"2025-04-19T07:44:10.058900Z","shell.execute_reply":"2025-04-19T07:44:10.063819Z"}},"outputs":[],"execution_count":18},{"cell_type":"code","source":"# Sample sentence\nenglish_sentence = \"My name is sarvagya\"\n\n# Translate\nfrench_translation = translate_english_to_french(model, tokenizer, english_sentence)\nprint(f\"French Translation: {french_translation}\")\n","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-04-19T07:44:39.384374Z","iopub.execute_input":"2025-04-19T07:44:39.384659Z","iopub.status.idle":"2025-04-19T07:44:39.509635Z","shell.execute_reply.started":"2025-04-19T07:44:39.384639Z","shell.execute_reply":"2025-04-19T07:44:39.509089Z"}},"outputs":[{"name":"stdout","text":"French Translation: Mon nom est sarvagya\n","output_type":"stream"}],"execution_count":20}]}