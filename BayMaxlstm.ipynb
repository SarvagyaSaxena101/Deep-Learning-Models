{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.10.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"},"kaggle":{"accelerator":"none","dataSources":[],"dockerImageVersionId":30918,"isInternetEnabled":true,"language":"python","sourceType":"notebook","isGpuEnabled":false}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"synopsis = \"\"\"\nHealthcare Assistant:\nSubmitted By -\nSubmitted To - \nSarvagya Saxena ( EA-2315800073)\nSneha Chaudhary(EA- 2315800079)\nSneha Agrawal(EA- 2315800078)\nSiddhartha Khandelwal(EB-2315800077)\nDr. Sayantan Sinha\nDepartment - Computer Application (AI and ML)\nIndex\nS.no.\nContent\nPage number\n1.\nIntroduction\n3-4\n2.\nSurvey of existing products\n4-6\n3.\nNeed of the project\n6-8\n4.\nObjective\n8-11\n5.\nHardware and Software requirements\n11-14\n6.\nMethodology\n14-16\n7.\nProject Planning\n16-17\n8.\nReferences\n17-18\nIntroduction \nHealthcare Assistant is an interactive humanoid robot that responds in real-time to help and interact with users using intelligent feedback and physical interaction. Drawing inspiration from the popular Disney character, the project seeks to develop a companion robot that can convey emotions, understand voice commands, and make simple movements. The emphasis is on improving human-robot interaction through the integration of AI, sensors, and motorized actuation.\nThe essence of Healthcare Assistant's functionality is in its smooth movement and meaningful responses to users. It employs a control system using [6] Arduino, combined with servo and gear motors, to facilitate realistic movements. Furthermore, AI-powered responses ensure that it can communicate well, hence being an interactive and engaging experience. The design focuses on stability and reliability to achieve seamless operation within real-time settings.\nThis project combines various technologies, such as microcontrollers, motor drivers, and AI algorithms, for an effective and interactive system. Future developments could include speech recognition, facial recognition, and autonomous navigation. Healthcare Assistant is seen as a multifunctional companion, with possibilities of application in healthcare and personal care.\nThe main goal is to develop Healthcare Assistant's skills further and introduce it in a future hackathon, demonstrating cutting-edge robotics and AI advancements. The project is a precursor to developing more intelligent and empathetic robots that can interact easily with humans in everyday life.\nSurvey of Existing Projects\nIn the Indian market for robotics, though industrial robots are in vogue, interest in healthcare robots to deliver affordable medical care is on the rise. Though such end-to-end solutions like Healthcare Assistant in Disney's \"Big Hero 6\" are yet to be the norm, various companies are going a long way in this regard:\n2.1 SS Innovations International Inc[1]: Established by Dr. Sudhir Srivastava, a world-renowned robotic cardiac surgeon, [1] SS Innovations is dedicated to creating sophisticated medical robotic systems. Their star product, the SSi Mantra, is intended to provide higher precision during surgery, leading to better patient outcomes.\n[1] SS INNOVATIONS INTERNATIONAL INC.\n2.2 DiFACTO Robotics and Automation[2]: With operations based in Bengaluru, [2] DiFACTO offers robotic solutions to industries such as healthcare. Its automation solutions focus on increasing efficiency and accuracy in medical use, which is used to drive healthcare technology improvements.\nELETIMES\n2.3 Asimov Robotics[3]: Founded in 2012, Asimov Robotics provides healthcare products and services, including patient support robots and medical training devices. Their solutions aim to enhance patient care and assist medical professionals.\nASIMOV ROBOTICS\n2.4 Theranautilus[4]: Bengaluru-based Theranautilus is a nanotechnology and robotics company that has come up with nanorobots to travel through dentinal tubules to reach bacterial infestations. This has the potential to reduce root canal failures, reflecting the future of robotics in precision medical treatments.\nWIKIPEDIA, THE FREE ENCYCLOPEDIA\n2.5 AgVa Healthcare[5]:\nIn a joint effort with AIIMS Delhi, AgVa Healthcare designed a low-cost and portable ventilator. The machine can be operated through an Android app, showcasing the convergence of robotics and digital technology to offer affordable healthcare solutions.\nWIKIPEDIA, THE FREE ENCYCLOPEDIA\nAlthough these developments are a sign of progress, the Indian market currently has limited products that are directly in line with the vision of offering healthcare support at one's fingertips in your project. This is an opportunity for innovation in designing easy-to-use, accessible healthcare robots specific to the Indian market.\nCompany / Product\nFeatures & Applications\nObservations (Compared to Healthcare Assistant)\nLimitations (Compared to Healthcare Assistant)\nSS Innovations - SSi Mantra\nSurgical robotics for precision surgery\nFocused on assisting surgeons rather than direct patient interaction\nNot designed for real-time patient assistance at home\nDiFACTO Robotics\nAutomation for medical procedures\nImproves medical efficiency but lacks a humanoid interface\nNot an interactive or AI-powered healthcare assistant\nAsimov Robotics\nPatient assistance robots, medical training\nClosest to Healthcare Assistant in concept, interactive robotic assistance\nLacks emotional AI and deep learning-based decision-making\nTheranautilus\nNanorobots for dental treatments\nNiche healthcare application, no general patient support\nLimited to dental use, no AI-based patient care\nAgVa Healthcare - AgVa Ventilator\nSmart ventilator controlled via an app\nShows potential for remote healthcare solutions\nLimited to ventilator functionality, not a humanoid companion\nHealthcare Assistant Project (Your Work)\nAI-driven healthcare assistant, real-time patient monitoring, emotional support, human-like interaction\nCombines AI, robotics, and emotional intelligence to provide proactive care\nRequires significant R&D, real-world implementation challenges\nTable 1: Study of existing solutions\nNeed of the Project \n3.1. Assistance During Pandemics\nDuring pandemics, such as COVID-19, human contact needs to be minimized to prevent the spread of infections. Healthcare assistants can play a crucial role in such situations by assisting healthcare professionals, delivering medicines, monitoring patients, and providing emotional support to those in isolation. Its ability to interact without physical contact reduces the risk of virus transmission, ensuring a safer environment for both patients and medical staff. Additionally, \nFurthermore, Healthcare Assistant can be programmed to track symptoms and provide virtual consultations, reducing the burden on overworked medical staff. Moreover, Healthcare Assistant can support mental health initiatives by providing companionship to isolated individuals, reducing the psychological impact of extended quarantines. Its integration with AI-driven diagnostics can also help detect early symptoms, allowing for timely intervention and treatment.\n3.2. Support for Elderly and Disabled Individuals\nElderly and disabled individuals often require constant support for daily tasks, which can be physically and mentally exhausting for caregivers. Healthcare Assistant can act as a personal assistant,providing mobility support, and engaging in conversations to alleviate loneliness. It can also monitor vital health parameters and alert caregivers in case of emergencies, ensuring timely medical intervention. By offering companionship and continuous assistance, Healthcare Assistant enhances the quality of life for individuals with special needs and reduces the burden on family members and healthcare providers.\nThrough voice commands and an intuitive interface, elderly users can interact with Healthcare Assistant seamlessly, improving their independence and reducing reliance on external caregivers. With its adaptive learning system.\n3.3. Enhancing Mental Health and Emotional Support\nMany people experience stress, anxiety, and depression due to social isolation, work pressure, or personal struggles. Healthcare Assistant can serve as a comforting presence by recognizing emotions, engaging in friendly conversations, and offering relaxation techniques. By integrating AI-driven emotional intelligence, it can provide personalized responses, reducing stress and promoting mental well-being. Furthermore, it can guide users through mindfulness exercises, breathing techniques, and motivational talks to improve their mental resilience. This function is especially beneficial for individuals experiencing loneliness or those undergoing therapy.\nBeyond basic conversation, Healthcare Assistant can assess emotional well-being using voice tone analysis, facial expressions, and behavioral patterns to provide deeper emotional support. It can recommend therapeutic activities, including guided meditation, calming music, or physical exercises tailored to the user’s needs. In workplaces, Healthcare Assistant can act as an AI-powered wellness coach, assisting employees with stress management techniques and promoting a healthier work-life balance. Additionally, for children dealing with social anxiety, Healthcare Assistant can act as a non-judgmental companion, helping them develop better communication and social interaction skills through AI-powered engagement exercises.\nObjective\n4.1. Enhance Healthcare Accessibility:\nHealthcare Assistant’s core objective is to enhance healthcare accessibility by providing real-time assistance, particularly for individuals who face difficulties in accessing traditional healthcare services. This includes people in remote or underserved areas, elderly individuals, or anyone with mobility issues. Healthcare Assistant can be programmed to interact with patients via a conversational interface, helping users monitor their health.  Through these features, Healthcare Assistant makes healthcare more accessible to people regardless of their geographic location, financial status, or mobility issues. The machine can act as an intermediary that provides guidance and support until more specialized healthcare interventions can be made available, improving both the speed and quality of healthcare access.\n4.2. Ensure User Comfort and Emotional Support:\nOne of Healthcare Assistant’s key features is its ability to provide emotional support, acting as both a healthcare assistant and a companion for individuals in need of reassurance or comfort. The robot is designed to be empathetic, understanding, and responsive to a wide range of emotions, making it particularly useful for individuals who experience anxiety, depression, or loneliness.\nConsider a situation where an elderly person lives alone and suffers from chronic illness. Healthcare Assistant can serve as a companion by providing friendly conversations, offering motivational support, and monitoring the individual’s mood. If Healthcare Assistant detects signs of sadness or distress through voice analysis or facial recognition, it could initiate a soothing dialogue, saying something like, “I can see that you’re feeling down. Would you like to talk about it?” Healthcare Assistant could even suggest activities to help lift the individual’s mood, such as listening to music or watching a favorite show.\nFurthermore, Healthcare Assistant could provide therapeutic interactions based on the user’s mental health needs. For someone experiencing anxiety or panic attacks, Healthcare Assistant might offer breathing exercises, calming techniques, or simply stay with the person, offering comforting words. The robot’s ability to engage emotionally with users not only enhances their experience but also reduces the emotional strain associated with physical illnesses or loneliness.\nFor children, Healthcare Assistant can also act as a comforting presence during medical visits or treatments. By interacting with them in a playful and gentle way, the robot can make the experience less intimidating, helping kids feel more relaxed and at ease with the medical process.\nHealthcare Assistant’s emphasis on comfort and emotional support is essential for providing a holistic approach to healthcare, where mental and emotional well-being is just as important as physical health. This feature allows Healthcare Assistant to be more than just a tool for medical tasks—it becomes a supportive companion, helping users feel cared for and less isolated.\n4.3. Improve Mobility and Responsiveness:\nAnother key objective of Healthcare Assistant is to be highly mobile and responsive, adapting to the needs of the user with precision and agility. This is crucial, as Healthcare Assistant’s ability to navigate diverse environments and respond promptly to different situations is integral to its functionality. Healthcare Assistant’s mobility and responsiveness are achieved through the integration of advanced sensors, actuators, and AI, which enable it to move efficiently and adapt to various challenges.\nThe robot's responsiveness is also key when it comes to interaction. Healthcare Assistant could use facial recognition to identify when a person is in distress or discomfort. If the robot notices that a user is struggling to breathe or experiencing difficulty moving, it would immediately alter its behavior, either offering assistance or activating the necessary protocols to help with mobility or health monitoring\n4.4. Foster Technological Innovation in Robotics:\nFinally, Healthcare Assistant aims to push the boundaries of robotics and AI to create a truly innovative healthcare assistant that could serve as a blueprint for future robots in medical settings. By integrating state-of-the-art technologies such as machine learning, computer vision, natural language processing, and real-time data analysis, Healthcare Assistant serves as a testing ground for innovative solutions to the challenges of personal healthcare and caregiving.\nFor instance, Healthcare Assistant could integrate real-time data analytics to track health trends, provide early detection of illnesses, and make informed predictions about future health conditions. Its sensors and AI would allow it to detect early signs of diseases such as diabetes, hypertension, or respiratory issues before they become critical, enabling proactive intervention.\nAdditionally, Healthcare Assistant could be used as a research platform for new robotic technologies, such as autonomous movement systems or adaptive interfaces. By constantly collecting data from its interactions with users, it can contribute to the development of more intelligent, responsive robots. In doing so, Healthcare Assistant could play a vital role in advancing the field of robotics, particularly in the realm of healthcare, where the demand for personalized care continues to grow.\nIn conclusion, Healthcare Assistant’s design and capabilities reflect a vision for the future of healthcare assistance, where technology not only aids in physical tasks but also enhances emotional well-being, mobility, and accessibility. Through continuous innovation and development, Healthcare Assistant stands to revolutionize the way we approach healthcare and caregiving.\nHardware and Software Requirements\n5.1 Hardware Requirements:\nThe hardware components of the Healthcare Assistant project are selected to provide a robust platform for real-time healthcare support, including health monitoring, emergency response, and autonomous mobility. These components allow Healthcare Assistant to interact with users, sense their environment, and respond intelligently to their needs.\n5.1.1. Raspberry Pi 4:\nThe [16] Raspberry Pi 4 serves as the central processing unit for Healthcare Assistant. With its powerful CPU and ample RAM (up to 8 GB), the [16] Raspberry Pi 4 provides the necessary computational power to run complex tasks such as real-time data analysis, machine learning models (including LLaMA 3), and communication between various hardware components. It also provides connectivity for Internet of Things (IoT) integration and remote control of Healthcare Assistant through Wi-Fi or Bluetooth using [8] Eleven Labs.\nRole: Central processing, computation, communication, and integration of external sensors.\n5.1.2. Arduin UNO R3o:\nThe [6] Arduino platform is used for controlling low-level hardware components such as motors, sensors, and actuators. It offers precise control over the movement of motors and the reading of sensor data. The [6]Arduino acts as a bridge between the sensors and the  [16] Raspberry Pi, ensuring seamless operation and responsiveness.\nRole: Motor control, sensor interfacing, all other sorts of movement \n5.1.3. Pi Camera V2:\nThe [10] Pi Camera V2 enables Healthcare Assistant to capture real-time video footage and process visual data. With its high-resolution capabilities, the camera is used for facial recognition, analyzing user expressions, and detecting physical conditions such as fatigue or distress. The camera helps facilitate the robot’s interaction with users, enabling it to respond to visual cues.\nRole: Facial recognition, emotion detection, video analysis, and user interaction.\n5.1.4. DTH11 Sensor:\nThe DTH11 sensor is a temperature and humidity sensor that will allow Healthcare Assistant to monitor the environment. It can be used to assess the temperature and humidity levels around and of a user, which is crucial in providing care and adjusting the robot’s behavior accordingly (e.g., suggesting warmth or cooling for comfort).\nRole: Environmental monitoring for comfort and health parameters.\n5.1.5. Motor Driver (L293D):\nThe L293D motor driver allows for controlling the geared motors and servo motors that power Healthcare Assistant’s movements. The motor driver controls both the direction and speed of the motors, ensuring smooth and precise operation of Healthcare Assistant’s physical features, such as head movement and body adjustments.\nRole: Motor control for mobility, precise movement, and task execution.\n5.1.6. Geared Motors:\nGeared motors are used to provide power for Healthcare Assistant's movement. These motors are responsible for enabling the robot to move autonomously and carry out specific tasks.\nRole: Mobility, movement, and task performance.\n5.1.7. Servo Motors:\nServo motors are employed in Healthcare Assistant to execute fine movements, such as head rotation, arm articulation, and other small but essential adjustments that add to the robot's ability to interact with users in a human-like manner. The servos are key for accurate motion control.\nRole: Fine-tuned movements for user interaction, task execution, and robotic gestures.\n5.1.8. Microphone (Mic) & Speaker:\nThe microphone and speaker allow Healthcare Assistant to engage in real-time voice interactions with users. The mic is used to capture audio for speech recognition, while the speaker enables Healthcare Assistant to respond with verbal feedback. This hardware is crucial for facilitating communication and emotional support.\nRole: Voice input for communication and speech output for responses.\n5.2 Software Requirements:\nThe software stack for Healthcare Assistant is designed to provide the necessary functionality for real-time health monitoring, data processing, user interaction, and machine learning. The combination of Python, FastAPI, LLaMA 3, Embedded C, and CNN ensures that Healthcare Assistant operates intelligently and efficiently in all its tasks.\n5.2.1. Python:\n[7] Python is used for the majority of high-level control and data processing. It is ideal for working with the [16] Raspberry Pi, running AI algorithms, ML-DL models, and handling user interaction through speech or text. \nRole: High-level control, data processing, AI integration, and user interaction.\n5.2.2. FastAPI:\nIt will be used to create fast and efficient communication between Healthcare Assistant and other servers . FastAPI’s asynchronous capabilities allow for real-time communication, making it suitable for Healthcare Assistant’s responsive features, such as health data monitoring and speech synthesis.\nRole: API development for communication between hardware components and real-time interactions.\n5.2.3. LLaMA 3:\nLLaMA 3 is a state-of-the-art language model that powers Healthcare Assistant’s ability to engage in intelligent and natural conversation with users. This software component will be essential for Healthcare Assistant to provide emotional support, answer medical queries, and offer companionship.\nRole: Natural language understanding, emotional support, and interaction.\n5.2.4. Embedded C:\nEmbedded C will be used to program the [6] Arduino and other low-level components. Embedded C allows for precise control of sensors, motors, and other hardware that Healthcare Assistant depends on for its mobility and functionality.\nRole: Low-level programming for sensor reading, motor control, and real-time interactions with hardware.\n5.2.5. Convolutional Neural Networks (CNN):\nCNNs will be used for image processing tasks, such as facial recognition and emotion detection. With the help of CNNs we will also analyze different types of scans (Bone X-Ray, Brain MRI , Pneumonia X-RAY), Healthcare Assistant can analyze visual data from the Pi \nRole: Image processing for facial recognition, emotion detection, and object recognition.\n6. Methodology  \n1. Problem Identification and Market Research\nThe initial phase involved identifying the gap in the Indian healthcare robotics market. Most available robots, such as industrial robotic arms and medical automation solutions, were designed for surgical precision or specific tasks rather than providing real-time, human-like patient assistance. Existing solutions lacked an emotionally intelligent, AI-powered healthcare assistant capable of interacting with patients in a natural, human-like manner.\nThrough market analysis and web scraping, we identified a lack of humanoid robots that integrate health monitoring, real-time assistance, and emotional support, validating the need for a solution like Healthcare Assistant.\n2. Hardware and Design Development\n2.1. Selection of Components\nProcessing Unit: [16] Raspberry Pi 4 (for AI processing and control)\nMicrocontroller: [6] Arduino (for motor control and real-time movements)\nSensors:\nTemperature sensors for health monitoring\nActuators: Servo motors for hand and head movement\nDisplay: OLED/LED screen for facial expressions and interaction\n2.2. Structural Design\nA 4-foot-tall humanoid robot designed with a soft-body shell, inspired by the conceptual Healthcare Assistant from Big Hero 6 but adapted for real-world applications.\nThe robot's outer shell was designed by wooden ply, cardboard and Satin cloth, to give it a steady yet soft look.\n3. AI & Software Integration\n3.1. Machine Learning and NLP for Communication\nImplemented Natural Language Processing (NLP) using GPT-based models for real-time conversation.\nIntegrated emotion detection algorithms to understand user sentiment and respond accordingly.\nSpeech synthesis using Google Text-to-Speech (TTS) APIs and [9] whisper ASR for speech recognition.\n3.2. Health Monitoring and Data Processing\nCollected real-time health parameters using attached sensors.\nImplemented deep learning models (CNNs, RNNs) to analyze sensor data for early health anomaly detection.\nAdditionally, he will prescribe a medication, which will subsequently be delivered in a tray.\n3.3. Motion and Gesture Control\nProgrammed motion planning using inverse kinematics for smooth and natural movements.\nConfigured gesture-based interactions using computer vision and OpenCV-based hand tracking.\n4. Human-Robot Interaction Testing\nOptimizing latency issues in sensor response and speech processing.\n5. Future Scope\nPlanned integration with telemedicine services, allowing users to connect with doctors via video/audio calls through the robot.\nFuture upgrades include self-learning capabilities, facial recognition for personalized healthcare, and autonomous movement within a hospital/home environment.\n7. Project Planning\n7.1 Project Initiation:\nDefine project goals, scope, and requirements.\nAssemble the team and assign responsibilities.\nEstablish project timeline and deliverables.\n7.2 Research & Design:\nResearch hardware components and ensure compatibility.\nDesign robot structure and functionalities.\nFinalize software architecture and integration plan.\n7.3 Hardware Assembly:\nAssemble [16] Raspberry Pi,[6] Arduino, sensors (ultrasonic, DTH11), motors, and camera.\nWire and mount components for mobility and sensing.\nSet up power supply system and ensure hardware connectivity.\n7.4 Software Development:\nSet up development environment (Python, FastAPI, Embedded C).\nDevelop code for sensor integration and motor control.\nImplement LLaMA 3 for conversational AI and integrate facial/emotion recognition using CNN.\n7.5 Integration and Testing:\nIntegrate hardware and software for full functionality.\nTest motors, sensors, LLaMA 3, and camera systems.\nPerform system-wide testing to ensure reliability and responsiveness.\n7.6 Optimization and Refinement:\nOptimize performance (speed, accuracy, battery management).\nImprove sensor calibration and AI response time.\nIncorporate user feedback and fine-tune emotional support features.\n7.7 Final Review:\nConduct a final review of the project’s functionality.\nPrepare documentation and testing reports.\n8 . References\n[1] SS Innovations - https://ssinnovations.com/\n[2] DiFACTO Robotics and Automation- https://difacto.com/\n[3] Asimov Robotics -https://asimovrobotics.com/\n[4] Theranautilus- https://theranautilus.com/\n[5] AgVa Healthcare- https://www.agvac.in/\n[6] Arduino - https://docs.arduino.cc/\n[7] Python - https://docs.python.org/3/\n[8] Eleven Labs - https://elevenlabs.io/docs/overview\n[9] Whisper - https://platform.openai.com/docs/guides/speech-to-text\n[10] Pi Camera- https://www.raspberrypi.com/documentation/computers/camera\n[11] LLM - https://python.langchain.com/docs/introduction/\n[12] ASR model - https://huggingface.co/docs/transformers/en/tasks/asr\n[13] Hugging Face - https://huggingface.co/docs\n[14] CUDA - https://docs.nvidia.com/cuda/\n[15] PI OS - https://www.raspberrypi.com/documentation/computers/os.html\n[16] Raspberry Pi - https://www.raspberrypi.com/documentation/\n\"\"\"","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:46:33.230241Z","iopub.execute_input":"2025-03-20T06:46:33.230920Z","iopub.status.idle":"2025-03-20T06:46:33.256005Z","shell.execute_reply.started":"2025-03-20T06:46:33.230843Z","shell.execute_reply":"2025-03-20T06:46:33.250766Z"}},"outputs":[],"execution_count":1},{"cell_type":"code","source":"import tensorflow as tf\nfrom tensorflow.keras.preprocessing.text import Tokenizer","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:46:33.259473Z","iopub.execute_input":"2025-03-20T06:46:33.260314Z","iopub.status.idle":"2025-03-20T06:46:41.413367Z","shell.execute_reply.started":"2025-03-20T06:46:33.260251Z","shell.execute_reply":"2025-03-20T06:46:41.410543Z"}},"outputs":[],"execution_count":2},{"cell_type":"code","source":"tokenizer = Tokenizer()\ntokenizer.fit_on_texts([synopsis])\nsequences = []\nfor sentence in synopsis.split('\\n'):\n    tokenized_sentence=  tokenizer.texts_to_sequences([sentence])[0]\n    for i in range(1,len(tokenized_sentence)):\n        n_gram = tokenized_sentence[:i+1]\n        sequences.append(n_gram)\nmaxlen = max([len(x) for x in sequences])","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:46:41.417087Z","iopub.execute_input":"2025-03-20T06:46:41.418206Z","iopub.status.idle":"2025-03-20T06:46:41.457546Z","shell.execute_reply.started":"2025-03-20T06:46:41.418129Z","shell.execute_reply":"2025-03-20T06:46:41.456021Z"}},"outputs":[],"execution_count":3},{"cell_type":"code","source":"maxlen","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T07:35:16.873497Z","iopub.execute_input":"2025-03-20T07:35:16.874046Z","iopub.status.idle":"2025-03-20T07:35:16.881702Z","shell.execute_reply.started":"2025-03-20T07:35:16.874008Z","shell.execute_reply":"2025-03-20T07:35:16.880227Z"}},"outputs":[{"execution_count":11,"output_type":"execute_result","data":{"text/plain":"109"},"metadata":{}}],"execution_count":11},{"cell_type":"code","source":"from keras.preprocessing.sequence import pad_sequences\npadded_input_sequences = pad_sequences(sequences,maxlen+1,padding='pre')\nx = padded_input_sequences[:,:-1]\ny = padded_input_sequences[:,-1]","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:46:41.459381Z","iopub.execute_input":"2025-03-20T06:46:41.459887Z","iopub.status.idle":"2025-03-20T06:46:41.506172Z","shell.execute_reply.started":"2025-03-20T06:46:41.459828Z","shell.execute_reply":"2025-03-20T06:46:41.504707Z"}},"outputs":[],"execution_count":4},{"cell_type":"code","source":"from keras.utils import to_categorical\ny = to_categorical(y,num_classes=1083)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:46:41.507413Z","iopub.execute_input":"2025-03-20T06:46:41.507917Z","iopub.status.idle":"2025-03-20T06:46:41.542921Z","shell.execute_reply.started":"2025-03-20T06:46:41.507869Z","shell.execute_reply":"2025-03-20T06:46:41.541301Z"}},"outputs":[],"execution_count":5},{"cell_type":"code","source":"y.shape","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:46:41.544119Z","iopub.execute_input":"2025-03-20T06:46:41.544581Z","iopub.status.idle":"2025-03-20T06:46:41.558790Z","shell.execute_reply.started":"2025-03-20T06:46:41.544531Z","shell.execute_reply":"2025-03-20T06:46:41.557307Z"}},"outputs":[{"execution_count":6,"output_type":"execute_result","data":{"text/plain":"(3497, 1083)"},"metadata":{}}],"execution_count":6},{"cell_type":"code","source":"from tensorflow.keras.models import Sequential\nfrom tensorflow.keras.layers import Embedding,Dense,LSTM\nmodel = Sequential()\nmodel.add(Embedding(1083,200,input_length=108))\nmodel.add(LSTM(150))\nmodel.add(Dense(1083,activation=\"softmax\"))","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:46:41.560232Z","iopub.execute_input":"2025-03-20T06:46:41.560674Z","iopub.status.idle":"2025-03-20T06:46:41.659700Z","shell.execute_reply.started":"2025-03-20T06:46:41.560626Z","shell.execute_reply":"2025-03-20T06:46:41.655680Z"}},"outputs":[{"name":"stderr","text":"/usr/local/lib/python3.10/dist-packages/keras/src/layers/core/embedding.py:90: UserWarning: Argument `input_length` is deprecated. Just remove it.\n  warnings.warn(\n","output_type":"stream"}],"execution_count":7},{"cell_type":"code","source":"model.compile(loss=\"categorical_crossentropy\",optimizer=\"adam\",metrics=['accuracy'])\nmodel.summary()","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T06:46:41.662528Z","iopub.execute_input":"2025-03-20T06:46:41.664848Z","iopub.status.idle":"2025-03-20T06:46:41.729173Z","shell.execute_reply.started":"2025-03-20T06:46:41.664780Z","shell.execute_reply":"2025-03-20T06:46:41.727061Z"}},"outputs":[{"output_type":"display_data","data":{"text/plain":"\u001b[1mModel: \"sequential\"\u001b[0m\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\">Model: \"sequential\"</span>\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃\u001b[1m \u001b[0m\u001b[1mLayer (type)                        \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1mOutput Shape               \u001b[0m\u001b[1m \u001b[0m┃\u001b[1m \u001b[0m\u001b[1m        Param #\u001b[0m\u001b[1m \u001b[0m┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (\u001b[38;5;33mEmbedding\u001b[0m)                │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm (\u001b[38;5;33mLSTM\u001b[0m)                          │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (\u001b[38;5;33mDense\u001b[0m)                        │ ?                           │     \u001b[38;5;34m0\u001b[0m (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\">┏━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━━━━━━━━━━━━━┳━━━━━━━━━━━━━━━━━┓\n┃<span style=\"font-weight: bold\"> Layer (type)                         </span>┃<span style=\"font-weight: bold\"> Output Shape                </span>┃<span style=\"font-weight: bold\">         Param # </span>┃\n┡━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━━━━━━━━━━━━━╇━━━━━━━━━━━━━━━━━┩\n│ embedding (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Embedding</span>)                │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ lstm (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">LSTM</span>)                          │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n├──────────────────────────────────────┼─────────────────────────────┼─────────────────┤\n│ dense (<span style=\"color: #0087ff; text-decoration-color: #0087ff\">Dense</span>)                        │ ?                           │     <span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (unbuilt) │\n└──────────────────────────────────────┴─────────────────────────────┴─────────────────┘\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Total params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Total params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}},{"output_type":"display_data","data":{"text/plain":"\u001b[1m Non-trainable params: \u001b[0m\u001b[38;5;34m0\u001b[0m (0.00 B)\n","text/html":"<pre style=\"white-space:pre;overflow-x:auto;line-height:normal;font-family:Menlo,'DejaVu Sans Mono',consolas,'Courier New',monospace\"><span style=\"font-weight: bold\"> Non-trainable params: </span><span style=\"color: #00af00; text-decoration-color: #00af00\">0</span> (0.00 B)\n</pre>\n"},"metadata":{}}],"execution_count":8},{"cell_type":"code","source":"# model.fit(x,y,epochs=100)\n\ntext = \"I\"\nfor i in range(0,5):\n    token_text = tokenizer.texts_to_sequences([text])[0]\n    \n    padded = pad_sequences([token_text],maxlen= 109,padding='post')\n    \n    pos= np.argmax(model.predict(padded))\n    \n    for word, index in tokenizer.word_index.items():\n        if index == pos:\n            text += \" \" + word\n            print(text)","metadata":{"trusted":true,"execution":{"iopub.status.busy":"2025-03-20T07:41:41.707000Z","iopub.execute_input":"2025-03-20T07:41:41.707449Z","iopub.status.idle":"2025-03-20T07:41:42.228286Z","shell.execute_reply.started":"2025-03-20T07:41:41.707419Z","shell.execute_reply":"2025-03-20T07:41:42.227275Z"}},"outputs":[{"name":"stdout","text":"\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\nI 3\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 42ms/step\nI 3 3\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 44ms/step\nI 3 3 3\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 46ms/step\nI 3 3 3 3\n\u001b[1m1/1\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m0s\u001b[0m 43ms/step\nI 3 3 3 3 3\n","output_type":"stream"}],"execution_count":22}]}